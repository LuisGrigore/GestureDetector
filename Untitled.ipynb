{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddc54549-8c2c-4219-9330-03c7d5d14c4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in ./venv/lib/python3.12/site-packages (0.10.21)\n",
      "Requirement already satisfied: opencv-python in ./venv/lib/python3.12/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: absl-py in ./venv/lib/python3.12/site-packages (from mediapipe) (2.3.1)\n",
      "Requirement already satisfied: attrs>=19.1.0 in ./venv/lib/python3.12/site-packages (from mediapipe) (25.4.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in ./venv/lib/python3.12/site-packages (from mediapipe) (25.9.23)\n",
      "Requirement already satisfied: jax in ./venv/lib/python3.12/site-packages (from mediapipe) (0.7.1)\n",
      "Requirement already satisfied: jaxlib in ./venv/lib/python3.12/site-packages (from mediapipe) (0.7.1)\n",
      "Requirement already satisfied: matplotlib in ./venv/lib/python3.12/site-packages (from mediapipe) (3.10.7)\n",
      "Requirement already satisfied: numpy<2 in ./venv/lib/python3.12/site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in ./venv/lib/python3.12/site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in ./venv/lib/python3.12/site-packages (from mediapipe) (4.25.8)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in ./venv/lib/python3.12/site-packages (from mediapipe) (0.5.3)\n",
      "Requirement already satisfied: sentencepiece in ./venv/lib/python3.12/site-packages (from mediapipe) (0.2.1)\n",
      "Requirement already satisfied: CFFI>=1.0 in ./venv/lib/python3.12/site-packages (from sounddevice>=0.4.4->mediapipe) (2.0.0)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in ./venv/lib/python3.12/site-packages (from jax->mediapipe) (0.5.4)\n",
      "Requirement already satisfied: opt_einsum in ./venv/lib/python3.12/site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.12 in ./venv/lib/python3.12/site-packages (from jax->mediapipe) (1.16.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.12/site-packages (from matplotlib->mediapipe) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.12/site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.12/site-packages (from matplotlib->mediapipe) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./venv/lib/python3.12/site-packages (from matplotlib->mediapipe) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.12/site-packages (from matplotlib->mediapipe) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./venv/lib/python3.12/site-packages (from matplotlib->mediapipe) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in ./venv/lib/python3.12/site-packages (from matplotlib->mediapipe) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.12/site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in ./venv/lib/python3.12/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.23)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "834c6e73-a4f7-4059-9b4e-ccde91a5b69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import uuid\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880c9683",
   "metadata": {},
   "source": [
    "# Hand Landmark Preprocessing Pipeline\n",
    "\n",
    "This notebook cell contains a set of functions to **preprocess hand landmarks**.  \n",
    "The preprocessing pipeline includes the following steps:\n",
    "\n",
    "1. **Extract coordinates** – Convert each hand landmark into a flat array of `(x, y, z)` values.  \n",
    "2. **Normalize wrist position** – Shift all landmarks so that the wrist (the first landmark) is at the origin `(0,0,0)`.  \n",
    "3. **Scale according to hand size** – Adjust the landmarks so that hand size differences do not affect the analysis.  \n",
    "4. **Normalize values** – Scale all coordinates to a fixed range (e.g., `[-1, 1]`) for consistent model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c253e9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_landmarks(hand_landmarks):\n",
    "    \"\"\"\n",
    "    Extracts the x, y, z coordinates from each landmark of the hand.\n",
    "\n",
    "    Parameters:\n",
    "        hand_landmarks: The hand landmarks object from a hand tracking model.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Flattened array containing all landmark coordinates [x1, y1, z1, x2, y2, z2, ...].\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for lm in hand_landmarks.landmark:\n",
    "        data.extend([lm.x, lm.y, lm.z])\n",
    "    return np.array(data)\n",
    "\n",
    "\n",
    "def normalize_landmarks(landmarks):\n",
    "    \"\"\"\n",
    "    Shifts all landmarks so that the wrist (first landmark) is at the origin (0,0,0).\n",
    "\n",
    "    Parameters:\n",
    "        landmarks (np.array): Flattened array of hand landmarks [x1, y1, z1, ...].\n",
    "\n",
    "    Returns:\n",
    "        np.array: Landmarks translated relative to the wrist position.\n",
    "    \"\"\"\n",
    "    landmarks = landmarks.copy()\n",
    "    base_x, base_y, base_z = landmarks[0], landmarks[1], landmarks[2]\n",
    "    for i in range(0, len(landmarks), 3):\n",
    "        landmarks[i] -= base_x\n",
    "        landmarks[i+1] -= base_y\n",
    "        landmarks[i+2] -= base_z\n",
    "    return landmarks\n",
    "\n",
    "def scale_landmarks(landmarks):\n",
    "    \"\"\"\n",
    "    Normalizes the scale of the hand landmarks based on the distance from wrist to middle finger MCP joint.\n",
    "\n",
    "    Parameters:\n",
    "        landmarks (np.array): Wrist-normalized landmarks.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Landmarks scaled to have a consistent hand size.\n",
    "    \"\"\"\n",
    "    wrist = np.array([landmarks[0], landmarks[1], landmarks[2]])\n",
    "    mcp = np.array([landmarks[27], landmarks[28], landmarks[29]])\n",
    "\n",
    "    scale = np.linalg.norm(mcp - wrist)\n",
    "    return landmarks / scale\n",
    "\n",
    "def minmax_landmarks(landmarks):\n",
    "    \"\"\"\n",
    "    Normalizes all landmark values to the range [-1, 1] based on the maximum absolute value.\n",
    "\n",
    "    Parameters:\n",
    "        landmarks (np.array): Scaled landmarks.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Landmarks normalized to [-1, 1].\n",
    "    \"\"\"\n",
    "    max_val = np.max(np.abs(landmarks))\n",
    "    return landmarks / max_val\n",
    "\n",
    "def preprocess_landmarks(hand_landmarks):\n",
    "    \"\"\"\n",
    "    Applies the full preprocessing pipeline:\n",
    "    1. Extracts landmarks\n",
    "    2. Normalizes wrist position\n",
    "    3. Scales landmarks according to hand size\n",
    "    4. Normalizes values to [-1, 1]\n",
    "\n",
    "    Parameters:\n",
    "        hand_landmarks: The hand landmarks object.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Fully preprocessed landmark array ready for model input.\n",
    "    \"\"\"\n",
    "    landmarks = extract_landmarks(hand_landmarks)\n",
    "    landmarks = normalize_landmarks(landmarks)\n",
    "    landmarks = scale_landmarks(landmarks)\n",
    "    landmarks = minmax_landmarks(landmarks)\n",
    "    return landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76604c91",
   "metadata": {},
   "source": [
    "# Camera Frame Capture Function\n",
    "\n",
    "This function captures a single frame from a webcam or video stream using OpenCV and performs basic preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6de637bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_camera_frame(cap:cv2.VideoCapture) -> np.ndarray|None:\n",
    "    \"\"\"\n",
    "    Captures a frame from the given video capture object and flips it horizontally.\n",
    "\n",
    "    Parameters:\n",
    "        cap (cv2.VideoCapture): OpenCV video capture object, usually created with `cv2.VideoCapture(0)`.\n",
    "\n",
    "    Returns:\n",
    "        frame (np.array or None): The captured frame with horizontal flip applied, or\n",
    "                                  None if the frame could not be read.\n",
    "    \"\"\"\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        return None\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0203ed2b",
   "metadata": {},
   "source": [
    "# Hand Landmark Drawing and Overlay Function\n",
    "\n",
    "This function draws hand landmarks on a video frame and overlays the processed landmark coordinates as text annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc05b654",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands_connections = mp.solutions.hands.HAND_CONNECTIONS\n",
    "\n",
    "def draw_overlays(frame, hand_landmarks, processed_landmarks):\n",
    "    \"\"\"\n",
    "    Draws hand landmarks and overlays processed coordinates on a given frame.\n",
    "\n",
    "    Parameters:\n",
    "        frame (np.array): The image frame on which to draw.\n",
    "        hand_landmarks (mp.framework.formats.landmark_pb2.NormalizedLandmarkList): \n",
    "            Hand landmarks detected by MediaPipe.\n",
    "        processed_landmarks (np.array): Preprocessed landmark coordinates from the pipeline.\n",
    "\n",
    "    Returns:\n",
    "        np.array: The frame with landmarks and coordinate annotations drawn.\n",
    "    \"\"\"\n",
    "    h, w, _ = frame.shape\n",
    "    mp_drawing.draw_landmarks(\n",
    "        frame, hand_landmarks, mp_hands_connections,\n",
    "\t\tmp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "\t\tmp_drawing.DrawingSpec(color=(250,44,250), thickness=2, circle_radius=2)\n",
    "  )\n",
    "    for idx, lm in enumerate(hand_landmarks.landmark):\n",
    "        cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "        px, py = processed_landmarks[idx*3], processed_landmarks[idx*3 + 1]\n",
    "        text = f\"{idx}:({px:.2f},{py:.2f})\"\n",
    "        cv2.putText(frame, text, (cx + 5, cy - 5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.35, (0,255,255), 1, cv2.LINE_AA)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c98a308",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from contextlib import AbstractContextManager\n",
    "from typing import Callable, Protocol, TypeVar\n",
    "\n",
    "ContextType = TypeVar(\"ContextType\", bound=AbstractContextManager)\n",
    "\n",
    "def cv_frame_loop(\n",
    "    cap: cv2.VideoCapture,\n",
    "    context: ContextType,\n",
    "    loop: Callable[[ContextType, np.ndarray], None],\n",
    "    flip: bool = True,\n",
    ") -> None:\n",
    "\tfps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\tdelay = int(1000 / fps) if fps > 0 else 1\n",
    "\ttry:\n",
    "\t\twith context:\n",
    "\t\t\twhile cap.isOpened():\n",
    "\t\t\t\tret, frame = cap.read()\n",
    "\t\t\t\tif not ret:\n",
    "\t\t\t\t\treturn None\n",
    "\t\t\t\tif flip:\n",
    "\t\t\t\t\tframe = cv2.flip(frame, 1)\n",
    "\t\t\t\tif frame is None:\n",
    "\t\t\t\t\tprint(\"No se puede leer la cámara.\")\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\tloop(context, frame)\n",
    "\t\t\t\tcv2.imshow(\"VideoInput\", frame)\n",
    "\t\t\t\tif cv2.waitKey(delay) & 0xFF == ord(\"q\"):\n",
    "\t\t\t\t\tbreak\n",
    "\tfinally:\n",
    "\t\tcap.release()\n",
    "\t\tcv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0ed19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List,cast\n",
    "from mediapipe.python.solutions.hands import Hands\n",
    "\n",
    "class SafeHandsResult(Protocol):\n",
    "    multi_hand_landmarks: List[Any] | None\n",
    "    \n",
    "class SafeHands(AbstractContextManager):\n",
    "\tdef __init__(self, *args, **kwargs):\n",
    "\t\tself.hands = Hands(*args, **kwargs)\n",
    "\t\n",
    "\tdef __enter__(self) -> Hands:\n",
    "\t\treturn self.hands\n",
    "\t\n",
    "\tdef __exit__(self, exc_type, exc_value, traceback) -> None:\n",
    "\t\tself.hands.close()\n",
    "\n",
    "\tdef process(self, frame_rgb: np.ndarray) -> SafeHandsResult:\n",
    "\t\treturn cast(SafeHandsResult, self.hands.process(frame_rgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6fe044cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1764978490.996635   41546 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1764978490.998903   48047 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.0.7-0ubuntu0.24.04.2), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1764978491.028094   48037 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1764978491.039397   48044 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "hands_model = SafeHands(min_detection_confidence=0.8, min_tracking_confidence=0.5)\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "def loop(hands: SafeHands, frame: np.ndarray) -> None:\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            processed = preprocess_landmarks(hand_landmarks)\n",
    "            frame = draw_overlays(frame, hand_landmarks, processed)\n",
    "\n",
    "cv_frame_loop(\n",
    "    cap=video_capture,\n",
    "    context=hands_model,\n",
    "    loop=loop,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2864fa6",
   "metadata": {},
   "source": [
    "# Real-Time Hand Landmark Preprocessing Visualization\n",
    "\n",
    "This cell captures video from the webcam, processes hand landmarks in real-time, and visualizes both the original landmarks and the preprocessed coordinates.\n",
    "\n",
    "## Main Components\n",
    "\n",
    "1. **Video Capture**\n",
    "    - `cv2.VideoCapture(0)` initializes the webcam.  \n",
    "    - Frames are continuously captured in a loop until the user quits.\n",
    "\n",
    "2. **Hand Detection**\n",
    "    - MediaPipe's `Hands` module is used with:\n",
    "        - `min_detection_confidence=0.8`\n",
    "        - `min_tracking_confidence=0.5`  \n",
    "    - The `hands.process(frame_rgb)` method detects hand landmarks in each frame.\n",
    "\n",
    "3. **Preprocessing Pipeline**\n",
    "    - For each detected hand:\n",
    "        1. Extract landmarks.\n",
    "        2. Normalize wrist position.\n",
    "        3. Scale landmarks relative to hand size.\n",
    "        4. Normalize values to the range [-1, 1].\n",
    "    - This is done using the `preprocess_landmarks` function.\n",
    "\n",
    "4. **Overlay Visualization**\n",
    "    - `draw_overlays` draws both:\n",
    "        - MediaPipe hand landmarks.\n",
    "        - Preprocessed coordinates as text annotations.\n",
    "\n",
    "5. **Display**\n",
    "    - The annotated frame is shown in a window using `cv2.imshow`.\n",
    "    - Press `q` to quit the visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e10a890-10c6-4b94-aa35-be818896d8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1764975777.017385   41546 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1764975777.019576   41682 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.0.7-0ubuntu0.24.04.2), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1764975777.050173   41675 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1764975777.069498   41675 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "mp_hands_module = mp.solutions.hands.Hands\n",
    "   \n",
    "def main():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    with mp_hands_module(min_detection_confidence=0.8, min_tracking_confidence=0.5) as hands:\n",
    "        while cap.isOpened():\n",
    "            frame = get_camera_frame(cap)\n",
    "            if frame is None:\n",
    "                print(\"No se puede leer la cámara.\")\n",
    "                break\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = hands.process(frame_rgb)\n",
    "\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    processed = preprocess_landmarks(hand_landmarks)\n",
    "                    frame = draw_overlays(frame, hand_landmarks, processed)\n",
    "\n",
    "            cv2.imshow(\"Webcam\", frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
