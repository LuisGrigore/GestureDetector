{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ddc54549-8c2c-4219-9330-03c7d5d14c4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in ./venv/lib/python3.12/site-packages (0.10.21)\n",
      "Requirement already satisfied: opencv-python in ./venv/lib/python3.12/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: pandas in ./venv/lib/python3.12/site-packages (2.3.3)\n",
      "Requirement already satisfied: tensorflow in ./venv/lib/python3.12/site-packages (2.19.1)\n",
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.12/site-packages (1.7.2)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: absl-py in ./venv/lib/python3.12/site-packages (from mediapipe) (2.3.1)\n",
      "Requirement already satisfied: attrs>=19.1.0 in ./venv/lib/python3.12/site-packages (from mediapipe) (25.4.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in ./venv/lib/python3.12/site-packages (from mediapipe) (25.9.23)\n",
      "Requirement already satisfied: jax in ./venv/lib/python3.12/site-packages (from mediapipe) (0.7.1)\n",
      "Requirement already satisfied: jaxlib in ./venv/lib/python3.12/site-packages (from mediapipe) (0.7.1)\n",
      "Requirement already satisfied: matplotlib in ./venv/lib/python3.12/site-packages (from mediapipe) (3.10.7)\n",
      "Requirement already satisfied: numpy<2 in ./venv/lib/python3.12/site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in ./venv/lib/python3.12/site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in ./venv/lib/python3.12/site-packages (from mediapipe) (4.25.8)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in ./venv/lib/python3.12/site-packages (from mediapipe) (0.5.3)\n",
      "Requirement already satisfied: sentencepiece in ./venv/lib/python3.12/site-packages (from mediapipe) (0.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./venv/lib/python3.12/site-packages (from tensorflow) (0.7.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./venv/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./venv/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.12/site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (2.32.5)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.12/site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./venv/lib/python3.12/site-packages (from tensorflow) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./venv/lib/python3.12/site-packages (from tensorflow) (1.76.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (3.12.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (3.15.1)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in ./venv/lib/python3.12/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./venv/lib/python3.12/site-packages (from scikit-learn) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./venv/lib/python3.12/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./venv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Collecting filelock (from datasets)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: httpx<1.0.0 in ./venv/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Downloading multiprocess-0.70.18-py312-none-any.whl.metadata (7.5 kB)\n",
      "Collecting fsspec<=2025.10.0,>=2023.1.0 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting huggingface-hub<2.0,>=0.25.0 (from datasets)\n",
      "  Downloading huggingface_hub-1.2.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.12/site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./venv/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: anyio in ./venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in ./venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting shellingham (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting typer-slim (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Downloading typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: rich in ./venv/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (14.2.0)\n",
      "Requirement already satisfied: namex in ./venv/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in ./venv/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: CFFI>=1.0 in ./venv/lib/python3.12/site-packages (from sounddevice>=0.4.4->mediapipe) (2.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./venv/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./venv/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./venv/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.12/site-packages (from matplotlib->mediapipe) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.12/site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.12/site-packages (from matplotlib->mediapipe) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./venv/lib/python3.12/site-packages (from matplotlib->mediapipe) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in ./venv/lib/python3.12/site-packages (from matplotlib->mediapipe) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in ./venv/lib/python3.12/site-packages (from matplotlib->mediapipe) (3.2.5)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.1/75.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pycparser in ./venv/lib/python3.12/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.23)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in ./venv/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
      "Collecting click>=8.0.0 (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Downloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.6/511.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.0/201.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-1.2.1-py3-none-any.whl (520 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m520.9/520.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.18-py312-none-any.whl (150 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.3/150.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.9/193.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0mm\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading typer_slim-0.20.0-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.3/108.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (242 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.1/256.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.6/221.6 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.3/377.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, tqdm, shellingham, pyarrow, propcache, multidict, hf-xet, fsspec, frozenlist, filelock, dill, click, aiohappyeyeballs, yarl, typer-slim, multiprocess, aiosignal, huggingface-hub, aiohttp, datasets\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 click-8.3.1 datasets-4.4.1 dill-0.4.0 filelock-3.20.0 frozenlist-1.8.0 fsspec-2025.10.0 hf-xet-1.2.0 huggingface-hub-1.2.1 multidict-6.7.0 multiprocess-0.70.18 propcache-0.4.1 pyarrow-22.0.0 shellingham-1.5.4 tqdm-4.67.1 typer-slim-0.20.0 xxhash-3.6.0 yarl-1.22.0\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe opencv-python pandas tensorflow scikit-learn datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "834c6e73-a4f7-4059-9b4e-ccde91a5b69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import uuid\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1be5a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "a\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Vincent-luo/hagrid-mediapipe-hands\", split=\"train\", streaming=True)\n",
    "\n",
    "print(\"a\")\n",
    "\n",
    "# Tomar solo los primeros 100 elementos\n",
    "small_dataset = []\n",
    "for i, example in enumerate(dataset):\n",
    "    print(\"a\")\n",
    "    if i >= 1:\n",
    "        print(\"a\")\n",
    "        break\n",
    "    small_dataset.append(example)\n",
    "print(\"a\")\n",
    "print(len(small_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880c9683",
   "metadata": {},
   "source": [
    "# Hand Landmark Preprocessing Pipeline\n",
    "\n",
    "This notebook cell contains a set of functions to **preprocess hand landmarks**.  \n",
    "The preprocessing pipeline includes the following steps:\n",
    "\n",
    "1. **Extract coordinates** – Convert each hand landmark into a flat array of `(x, y, z)` values.  \n",
    "2. **Normalize wrist position** – Shift all landmarks so that the wrist (the first landmark) is at the origin `(0,0,0)`.  \n",
    "3. **Scale according to hand size** – Adjust the landmarks so that hand size differences do not affect the analysis.  \n",
    "4. **Normalize values** – Scale all coordinates to a fixed range (e.g., `[-1, 1]`) for consistent model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c253e9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_landmarks(landmarks: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Shifts all landmarks so that the wrist (first landmark) is at the origin (0,0,0).\n",
    "\n",
    "    Parameters:\n",
    "        landmarks: np.ndarray of shape [NUM_LANDMARKS, 3]\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Landmarks translated relative to the wrist.\n",
    "    \"\"\"\n",
    "    landmarks = landmarks.copy()\n",
    "    base = landmarks[0]\n",
    "    return landmarks - base\n",
    "\n",
    "\n",
    "def scale_landmarks(landmarks: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalizes the scale of the hand landmarks based on the distance from wrist to middle finger MCP joint.\n",
    "\n",
    "    Parameters:\n",
    "        landmarks: np.ndarray de landmarks normalizados por la muñeca [NUM_LANDMARKS, 3]\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Landmarks escalados a tamaño de mano consistente.\n",
    "    \"\"\"\n",
    "    wrist = landmarks[0]\n",
    "    mcp = landmarks[9]\n",
    "    scale = np.linalg.norm(mcp - wrist)\n",
    "    return landmarks / scale\n",
    "\n",
    "\n",
    "def minmax_landmarks(landmarks: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalizes all landmark values to the range [-1, 1] based on the maximum absolute value.\n",
    "\n",
    "    Parameters:\n",
    "        landmarks: np.ndarray de landmarks escalados\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Landmarks normalizados a [-1, 1]\n",
    "    \"\"\"\n",
    "    max_val = np.max(np.abs(landmarks))\n",
    "    return landmarks / max_val\n",
    "\n",
    "\n",
    "def preprocess_landmarks(hand_landmarks: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applies the full preprocessing pipeline, preserving [NUM_LANDMARKS, 3] shape.\n",
    "\n",
    "    Parameters:\n",
    "        hand_landmarks: Array de forma [21, 3]\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Fully preprocessed landmark array.\n",
    "    \"\"\"\n",
    "    landmarks = normalize_landmarks(hand_landmarks)\n",
    "    landmarks = scale_landmarks(landmarks)\n",
    "    landmarks = minmax_landmarks(landmarks)\n",
    "    return landmarks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0203ed2b",
   "metadata": {},
   "source": [
    "# Hand Landmark Drawing and Overlay Function\n",
    "\n",
    "This function draws hand landmarks on a video frame and overlays the processed landmark coordinates as text annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc05b654",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands_connections = mp.solutions.hands.HAND_CONNECTIONS\n",
    "\n",
    "def draw_overlays(frame, hand_landmarks_np, processed_landmarks=None):\n",
    "    \"\"\"\n",
    "    Draws hand landmarks and connections on a given frame.\n",
    "    Parameters:\n",
    "        frame (np.array): The image frame.\n",
    "        hand_landmarks_np (np.array): Numpy array of shape (21, 3) with hand landmarks.\n",
    "        processed_landmarks (np.array, optional): Preprocessed landmark coordinates [21,3].\n",
    "    \"\"\"\n",
    "    h, w, _ = frame.shape\n",
    "\n",
    "    for connection in mp_hands_connections:\n",
    "        start_idx, end_idx = connection\n",
    "        x0, y0 = int(hand_landmarks_np[start_idx][0] * w), int(hand_landmarks_np[start_idx][1] * h)\n",
    "        x1, y1 = int(hand_landmarks_np[end_idx][0] * w), int(hand_landmarks_np[end_idx][1] * h)\n",
    "        cv2.line(frame, (x0, y0), (x1, y1), (0, 255, 0), 2)\n",
    "\n",
    "    for idx, lm in enumerate(hand_landmarks_np):\n",
    "        cx, cy = int(lm[0] * w), int(lm[1] * h)\n",
    "        cv2.circle(frame, (cx, cy), 6, (0, 0, 255), -1)\n",
    "\n",
    "        if processed_landmarks is not None:\n",
    "            px, py = processed_landmarks[idx][0], processed_landmarks[idx][1]\n",
    "            text = f\"{idx}:({px:.2f},{py:.2f})\"\n",
    "            cv2.putText(frame, text, (cx + 5, cy - 5),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.35, (255, 255, 0), 1, cv2.LINE_AA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dfc8a0",
   "metadata": {},
   "source": [
    "# Flexible Video Capture Loop\n",
    "\n",
    "The `video_capture_loop` function:\n",
    "\n",
    "- Reads frames from a video source (`cv2.VideoCapture`)\n",
    "- Optionally flips frames horizontally or vertically\n",
    "- Processes frames using a user-defined function\n",
    "- Can display frames in a window or skip GUI for automated processing\n",
    "- Ensures proper resource cleanup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c98a308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import AbstractContextManager\n",
    "from typing import Callable, TypeVar\n",
    "import time\n",
    "\n",
    "ContextType = TypeVar(\"ContextType\", bound=AbstractContextManager)\n",
    "\n",
    "class ContextError(Exception):\n",
    "    pass\n",
    "\n",
    "class VideoCaptureError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def flip_frame(frame: np.ndarray, flipH: bool, flipV: bool) -> np.ndarray:\n",
    "    if flipH and flipV:\n",
    "        return cv2.flip(frame, -1)\n",
    "    elif flipH:\n",
    "        return cv2.flip(frame, 1) \n",
    "    elif flipV:\n",
    "        return cv2.flip(frame, 0)\n",
    "    return frame\n",
    "\n",
    "\n",
    "def video_capture_loop(\n",
    "    cap: cv2.VideoCapture,\n",
    "    context: ContextType,\n",
    "    loop: Callable[[ContextType, np.ndarray], bool],\n",
    "    flipH: bool = False,\n",
    "    flipV: bool = False,\n",
    "    show_window: bool = True,\n",
    "    show_fps: bool = False\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Continuously captures frames from a video source, processes them using a user-defined \n",
    "    function, optionally flips frames, optionally displays them in a window, and optionally\n",
    "    shows the current FPS on the video.\n",
    "    \"\"\"\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    delay = int(1000 / fps) if fps > 0 else 1\n",
    "    prev_time = time.time()\n",
    "\n",
    "    try:\n",
    "        with context as ctx:\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret or frame is None:\n",
    "                    total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "                    current_frame = cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "                    if total_frames > 0 and current_frame >= total_frames:\n",
    "                        break\n",
    "                    else:\n",
    "                        raise VideoCaptureError(\"Unable to read video capture.\")\n",
    "                \n",
    "                frame = flip_frame(frame, flipH, flipV)\n",
    "                \n",
    "                try:\n",
    "                    if loop(ctx, frame):\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    raise VideoCaptureError(\"Error while processing frame.\") from e\n",
    "                \n",
    "                if show_window:\n",
    "                    if show_fps:\n",
    "                        current_time = time.time()\n",
    "                        current_fps = 1.0 / (current_time - prev_time) if (current_time - prev_time) > 0 else 0\n",
    "                        prev_time = current_time\n",
    "                        cv2.putText(\n",
    "                            frame,\n",
    "                            f\"FPS: {current_fps:.2f}\",\n",
    "                            (10, 30),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            1,\n",
    "                            (0, 255, 0),\n",
    "                            2,\n",
    "                            cv2.LINE_AA\n",
    "                        )\n",
    "                    cv2.imshow(\"VideoInput\", frame)\n",
    "                    if cv2.waitKey(delay) & 0xFF == ord(\"q\"):\n",
    "                        break\n",
    "\n",
    "    except Exception as e:\n",
    "        if not isinstance(e, VideoCaptureError):\n",
    "            raise ContextError(\"Error while using context.\") from e\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    finally:\n",
    "        if cap.isOpened():\n",
    "            cap.release()\n",
    "        if show_window:\n",
    "            cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d0ed19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from asyncio import Protocol\n",
    "from contextlib import AbstractContextManager\n",
    "from typing import Optional, Sequence, Tuple, cast\n",
    "from mediapipe.python.solutions.hands import Hands\n",
    "\n",
    "\n",
    "class SafeHandsResult(Protocol):\n",
    "    multi_hand_landmarks: Optional[Sequence]\n",
    "    multi_hand_world_landmarks: Optional[Sequence]\n",
    "    multi_handedness: Optional[Sequence]\n",
    "\n",
    "class SafeHands(AbstractContextManager):\n",
    "\tdef __init__(self, \n",
    "              \tstatic_image_mode: bool = False,\n",
    "\t\t\t    max_num_hands: int = 2,\n",
    "\t\t\t    model_complexity: int = 1,\n",
    "\t\t\t    min_detection_confidence: float = 0.5,\n",
    "\t\t\t    min_tracking_confidence: float = 0.5):\n",
    "\t\tself.max_num_hands = max_num_hands\n",
    "\t\tself.hands = Hands(static_image_mode = static_image_mode,\n",
    "\t\t\t    max_num_hands = max_num_hands,\n",
    "\t\t\t    model_complexity = model_complexity,\n",
    "\t\t\t    min_detection_confidence = min_detection_confidence,\n",
    "\t\t\t    min_tracking_confidence = min_tracking_confidence)\n",
    "    \n",
    "\tdef __enter__(self) -> \"SafeHands\":\n",
    "\t\treturn self\n",
    "    \n",
    "\tdef __exit__(self, exc_type, exc_value, traceback) -> None:\n",
    "\t\tself.hands.close()\n",
    "\n",
    "\tdef process(self, frame_rgb: np.ndarray) -> Optional[Tuple[np.ndarray, np.ndarray, np.ndarray]]:\n",
    "\t\tmp_result = cast(SafeHandsResult, self.hands.process(frame_rgb))\n",
    "\t\tif (not mp_result.multi_hand_landmarks and not mp_result.multi_hand_world_landmarks and not mp_result.multi_handedness):\n",
    "\t\t\treturn None\n",
    "\t\tlandmarks = np.zeros((self.max_num_hands, 21, 3), dtype=np.float32)\n",
    "\t\tworld_landmarks = np.zeros((self.max_num_hands, 21, 3), dtype=np.float32)\n",
    "\t\thandedness = np.zeros((self.max_num_hands, 2), dtype=np.float32)\n",
    "\t\tif mp_result.multi_hand_landmarks:\n",
    "\t\t\tfor i, hand in enumerate(mp_result.multi_hand_landmarks[:self.max_num_hands]):\n",
    "\t\t\t\tfor j, lm in enumerate(hand.landmark):\n",
    "\t\t\t\t\tlandmarks[i, j, 0] = lm.x\n",
    "\t\t\t\t\tlandmarks[i, j, 1] = lm.y\n",
    "\t\t\t\t\tlandmarks[i, j, 2] = lm.z\n",
    "\n",
    "\t\tif mp_result.multi_hand_world_landmarks:\n",
    "\t\t\tfor i, hand in enumerate(mp_result.multi_hand_world_landmarks[:self.max_num_hands]):\n",
    "\t\t\t\tfor j, lm in enumerate(hand.landmark):\n",
    "\t\t\t\t\tworld_landmarks[i, j, 2] = lm.z\n",
    "\t\t\t\t\tworld_landmarks[i, j, 0] = lm.x\n",
    "\t\t\t\t\tworld_landmarks[i, j, 1] = lm.y\n",
    "\n",
    "\t\tif mp_result.multi_handedness:\n",
    "\t\t\tfor i, h in enumerate(mp_result.multi_handedness[:self.max_num_hands]):\n",
    "\t\t\t\thandedness[i, 0] = 0 if h.classification[0].label == \"Left\" else 1\n",
    "\t\t\t\thandedness[i, 1] = h.classification[0].score\n",
    "\n",
    "\t\treturn landmarks, world_landmarks, handedness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91e6b474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1765058423.054137  147411 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1765058423.056694  154889 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.0.7-0ubuntu0.24.04.2), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1765058423.091659  154881 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1765058423.099036  154884 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando ejemplo 1...\n",
      "Procesando ejemplo 2...\n",
      "Procesando ejemplo 3...\n",
      "Procesando ejemplo 4...\n",
      "Procesando ejemplo 5...\n",
      "Procesando ejemplo 6...\n",
      "Procesando ejemplo 7...\n",
      "Procesando ejemplo 8...\n",
      "Procesando ejemplo 9...\n",
      "Procesando ejemplo 10...\n",
      "Procesando ejemplo 11...\n",
      "Procesando ejemplo 12...\n",
      "Procesando ejemplo 13...\n",
      "Procesando ejemplo 14...\n",
      "Procesando ejemplo 15...\n",
      "Procesando ejemplo 16...\n",
      "Procesando ejemplo 17...\n",
      "Procesando ejemplo 18...\n",
      "Procesando ejemplo 19...\n",
      "Procesando ejemplo 20...\n",
      "Procesando ejemplo 21...\n",
      "Procesando ejemplo 22...\n",
      "Procesando ejemplo 23...\n",
      "Procesando ejemplo 24...\n",
      "Procesando ejemplo 25...\n",
      "Procesando ejemplo 26...\n",
      "Procesando ejemplo 27...\n",
      "Procesando ejemplo 28...\n",
      "Procesando ejemplo 29...\n",
      "Procesando ejemplo 30...\n",
      "Procesando ejemplo 31...\n",
      "Procesando ejemplo 32...\n",
      "Procesando ejemplo 33...\n",
      "Procesando ejemplo 34...\n",
      "Procesando ejemplo 35...\n",
      "Procesando ejemplo 36...\n",
      "Procesando ejemplo 37...\n",
      "Procesando ejemplo 38...\n",
      "Procesando ejemplo 39...\n",
      "Procesando ejemplo 40...\n",
      "Procesando ejemplo 41...\n",
      "Procesando ejemplo 42...\n",
      "Procesando ejemplo 43...\n",
      "Procesando ejemplo 44...\n",
      "Procesando ejemplo 45...\n",
      "Procesando ejemplo 46...\n",
      "Procesando ejemplo 47...\n",
      "Procesando ejemplo 48...\n",
      "Procesando ejemplo 49...\n",
      "Procesando ejemplo 50...\n",
      "Procesando ejemplo 51...\n",
      "                                           landmarks  \\\n",
      "0  [[[0.22591458, 0.8506378, -8.855681e-07], [0.2...   \n",
      "1  [[[0.5153034, 0.47745067, -3.3627583e-07], [0....   \n",
      "2  [[[0.47015536, 0.57610345, -1.621919e-06], [0....   \n",
      "3  [[[0.47417098, 0.8300825, 5.2950986e-07], [0.4...   \n",
      "4  [[[0.7354868, 0.5422314, -9.1142766e-08], [0.7...   \n",
      "\n",
      "                                     world_landmarks  \\\n",
      "0  [[[-0.050177753, 0.038087796, 0.03814269], [-0...   \n",
      "1  [[[-0.079834275, 0.000120338984, 0.049313113],...   \n",
      "2  [[[-0.023964863, 0.07695302, 0.04091716], [0.0...   \n",
      "3  [[[0.0054369983, 0.09125657, 0.051100105], [-0...   \n",
      "4  [[[0.0460677, 0.02068854, 0.066266656], [0.035...   \n",
      "\n",
      "                        handedness  \n",
      "0   [[0.0, 0.9611227], [0.0, 0.0]]  \n",
      "1   [[0.0, 0.9966782], [0.0, 0.0]]  \n",
      "2   [[0.0, 0.9864846], [0.0, 0.0]]  \n",
      "3   [[1.0, 0.9893962], [0.0, 0.0]]  \n",
      "4  [[1.0, 0.99821657], [0.0, 0.0]]  \n",
      "Procesadas 48 imágenes y guardadas en 'hands_landmarks.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# Configuración del dataset\n",
    "# ----------------------------\n",
    "\n",
    "dataset_name = \"Vincent-luo/hagrid-mediapipe-hands\"\n",
    "split_name = \"train\"\n",
    "N = 50  # número de imágenes a procesar\n",
    "\n",
    "# Cargar dataset en streaming\n",
    "dataset = load_dataset(dataset_name, split=split_name, streaming=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Procesar imágenes y guardar en DataFrame\n",
    "# ----------------------------\n",
    "\n",
    "rows = []\n",
    "\n",
    "with SafeHands(static_image_mode=True, max_num_hands=2) as safe_hands:\n",
    "    for i, item in enumerate(dataset):\n",
    "        print(f\"Procesando ejemplo {i+1}...\")\n",
    "        if i >= N:\n",
    "            break\n",
    "\n",
    "        # Convertir a RGB np.array\n",
    "        img = item['image']\n",
    "        frame_rgb = np.array(img.convert('RGB'))\n",
    "\n",
    "        # Procesar con mediapipe\n",
    "        result = safe_hands.process(frame_rgb)\n",
    "\n",
    "        if result is not None:\n",
    "            landmarks, world_landmarks, handedness = result\n",
    "\n",
    "            # Guardar como fila en DataFrame (puedes modificar la estructura)\n",
    "            rows.append({\n",
    "                'landmarks': landmarks,\n",
    "                'world_landmarks': world_landmarks,\n",
    "                'handedness': handedness\n",
    "            })\n",
    "\n",
    "        # Liberar memoria de la imagen\n",
    "        del img, frame_rgb\n",
    "\n",
    "# Crear DataFrame final\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# Guardar a disco (opcional)\n",
    "df.to_pickle(\"hands_landmarks.pkl\")\n",
    "\n",
    "print(f\"Procesadas {len(df)} imágenes y guardadas en 'hands_landmarks.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e964c059",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1765037323.522073  119354 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1765037323.527498  123871 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.0.7-0ubuntu0.24.04.2), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1765037323.542757  123865 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1765037323.549784  123863 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muestra 0 guardada en gestures_dataset/none\n",
      "Muestra 1 guardada en gestures_dataset/none\n",
      "Muestra 2 guardada en gestures_dataset/none\n",
      "Muestra 3 guardada en gestures_dataset/none\n",
      "Muestra 4 guardada en gestures_dataset/none\n",
      "Muestra 5 guardada en gestures_dataset/none\n",
      "Muestra 6 guardada en gestures_dataset/none\n",
      "Muestra 7 guardada en gestures_dataset/none\n",
      "Muestra 8 guardada en gestures_dataset/none\n",
      "Muestra 9 guardada en gestures_dataset/none\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "DATASET_DIR = Path(\"gestures_dataset\")\n",
    "DATASET_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "GESTURE_LABEL = input(\"Introduce el nombre del gesto: \").strip()\n",
    "GESTURE_DIR = DATASET_DIR / GESTURE_LABEL\n",
    "GESTURE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "MAX_HANDS = 1\n",
    "IMG_SIZE = 224\n",
    "\n",
    "existing_files = list(GESTURE_DIR.glob(f\"{GESTURE_LABEL}_*.png\"))\n",
    "sample_idx = len(existing_files)\n",
    "\n",
    "def save_sample(image: np.ndarray, landmarks: np.ndarray, world_landmarks: np.ndarray, handedness: np.ndarray, idx: int):\n",
    "    img_path = GESTURE_DIR / f\"{GESTURE_LABEL}_{idx:03d}.png\"\n",
    "    cv2.imwrite(str(img_path), image)\n",
    "    np.savez(GESTURE_DIR / f\"{GESTURE_LABEL}_{idx:03d}_landmarks.npz\",\n",
    "             landmarks=landmarks,\n",
    "             world_landmarks=world_landmarks,\n",
    "             handedness=handedness)\n",
    "    print(f\"Muestra {idx} guardada en {GESTURE_DIR}\")\n",
    "\n",
    "save_next = False\n",
    "\n",
    "def loop_fn(ctx: SafeHands, frame: np.ndarray):\n",
    "    global sample_idx, save_next\n",
    "\n",
    "    result = ctx.process(frame)\n",
    "    if result is None:\n",
    "        return False\n",
    "\n",
    "    landmarks, world_landmarks, handedness = result\n",
    "    lm = landmarks[0]\n",
    "    if not lm.any():\n",
    "        return False\n",
    "\n",
    "    h, w, _ = frame.shape\n",
    "    margin_ratio = 1  # 10% de margen\n",
    "\n",
    "    # Centro de la mano\n",
    "    x_center = int(np.mean(lm[:, 0]) * w)\n",
    "    y_center = int(np.mean(lm[:, 1]) * h )\n",
    "\n",
    "    # Tamaño fijo del recorte (ajústalo según tu necesidad)\n",
    "    base_size = 200  # la mano siempre ocupará este tamaño aproximado\n",
    "    side = int(base_size * (1 + margin_ratio))\n",
    "\n",
    "    # Coordenadas del recorte\n",
    "    x_min_s = max(x_center - side // 2, 0)\n",
    "    x_max_s = min(x_center + side // 2, w)\n",
    "    y_min_s = max(y_center - side // 2, 0)\n",
    "    y_max_s = min(y_center + side // 2, h)\n",
    "\n",
    "    if x_max_s <= x_min_s or y_max_s <= y_min_s:\n",
    "        return False\n",
    "\n",
    "    # Extraer recorte y escalar a tamaño fijo\n",
    "    hand_img = frame[y_min_s:y_max_s, x_min_s:x_max_s]\n",
    "    hand_img = cv2.resize(hand_img, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    cv2.imshow(\"Hand\", hand_img)\n",
    "\n",
    "    key = cv2.waitKey(30) & 0xFF\n",
    "    if key == ord(\" \"):\n",
    "        save_next = True\n",
    "    elif key == ord(\"q\"):\n",
    "        return True\n",
    "\n",
    "    if save_next:\n",
    "        save_sample(hand_img, landmarks, world_landmarks, handedness, sample_idx)\n",
    "        sample_idx += 1\n",
    "        save_next = False\n",
    "\n",
    "    return False\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "video_capture_loop(cap, SafeHands(static_image_mode=False, max_num_hands=MAX_HANDS), loop_fn, flipH=True, flipV=False, show_window=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d765ef13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    label  lm_0  lm_1  lm_2      lm_3      lm_4      lm_5      lm_6      lm_7  \\\n",
      "0    none   0.0   0.0   0.0  0.022362 -0.224860  0.048987 -0.013007 -0.399016   \n",
      "1    none   0.0   0.0   0.0 -0.168673 -0.197016  0.005464 -0.263432 -0.465957   \n",
      "2    none   0.0   0.0   0.0  0.034995 -0.262031  0.085633  0.009668 -0.457776   \n",
      "3    none   0.0   0.0   0.0 -0.315975  0.001227 -0.107589 -0.668066 -0.310615   \n",
      "4    none   0.0   0.0   0.0 -0.217001 -0.053938 -0.061331 -0.427230 -0.259845   \n",
      "5    none   0.0   0.0   0.0  0.067865 -0.331119  0.054693  0.034584 -0.638145   \n",
      "6    none   0.0   0.0   0.0 -0.277613  0.144229 -0.087469 -0.576833  0.145855   \n",
      "7    none   0.0   0.0   0.0 -0.316737 -0.032833 -0.059449 -0.598963 -0.212684   \n",
      "8    none   0.0   0.0   0.0 -0.213536 -0.106233 -0.045819 -0.392986 -0.334831   \n",
      "9   right   0.0   0.0   0.0  0.175702 -0.266518 -0.095048  0.276969 -0.565052   \n",
      "10  right   0.0   0.0   0.0  0.154882 -0.270428 -0.013469  0.265143 -0.586972   \n",
      "11  right   0.0   0.0   0.0  0.234075 -0.248305  0.014055  0.382423 -0.556383   \n",
      "12  right   0.0   0.0   0.0  0.129317 -0.282326 -0.005301  0.207449 -0.574704   \n",
      "13  right   0.0   0.0   0.0  0.270150 -0.120313 -0.022579  0.465376 -0.315991   \n",
      "14  right   0.0   0.0   0.0  0.160152 -0.271589 -0.027229  0.267439 -0.571897   \n",
      "15  right   0.0   0.0   0.0  0.156939 -0.255172  0.072164  0.179116 -0.487066   \n",
      "16  right   0.0   0.0   0.0  0.164074 -0.275648 -0.075528  0.271154 -0.569450   \n",
      "17  right   0.0   0.0   0.0  0.100941 -0.270914  0.036431  0.186757 -0.587679   \n",
      "18   left   0.0   0.0   0.0 -0.199484 -0.189259 -0.147916 -0.336924 -0.537360   \n",
      "19   left   0.0   0.0   0.0 -0.234077 -0.206254 -0.134392 -0.386171 -0.535766   \n",
      "20   left   0.0   0.0   0.0 -0.234647  0.034017 -0.154869 -0.555526 -0.007776   \n",
      "21   left   0.0   0.0   0.0 -0.270750 -0.066379 -0.190780 -0.568353 -0.252296   \n",
      "22   left   0.0   0.0   0.0 -0.252064 -0.125318 -0.108676 -0.499992 -0.411749   \n",
      "23   left   0.0   0.0   0.0 -0.289670 -0.134356 -0.145489 -0.532806 -0.453913   \n",
      "24   left   0.0   0.0   0.0 -0.282215 -0.087025 -0.111084 -0.565312 -0.355025   \n",
      "25   left   0.0   0.0   0.0 -0.298995 -0.065490 -0.030795 -0.580258 -0.323163   \n",
      "26   left   0.0   0.0   0.0 -0.316490 -0.076156 -0.001780 -0.577665 -0.338181   \n",
      "\n",
      "        lm_8  ...     lm_53     lm_54     lm_55     lm_56     lm_57     lm_58  \\\n",
      "0   0.040817  ... -0.172827 -0.317671 -0.447153 -0.195416 -0.378220 -0.548958   \n",
      "1   0.002606  ... -0.095032  0.076880 -0.686675 -0.153874  0.010111 -0.532643   \n",
      "2   0.096491  ... -0.183492 -0.353872 -0.540485 -0.200501 -0.430342 -0.627470   \n",
      "3  -0.122394  ... -0.065051 -0.244369 -0.889257 -0.148859 -0.204471 -0.597587   \n",
      "4  -0.079191  ... -0.005081 -0.060192 -0.918090 -0.049107 -0.110032 -0.961114   \n",
      "5   0.039405  ... -0.173320 -0.434584 -0.558906 -0.159672 -0.433452 -0.441487   \n",
      "6  -0.118223  ... -0.000137 -0.551198 -0.805668 -0.066175 -0.617999 -0.803055   \n",
      "7  -0.075905  ... -0.054718 -0.267700 -0.874190 -0.145928 -0.231510 -0.672550   \n",
      "8  -0.063340  ... -0.033366 -0.031697 -0.858057 -0.093554 -0.091675 -0.763549   \n",
      "9  -0.148234  ...  0.030296 -0.094929 -0.594280  0.017405 -0.039745 -0.512981   \n",
      "10 -0.046557  ... -0.070951 -0.380549 -0.484621 -0.045442 -0.329962 -0.399807   \n",
      "11  0.001754  ... -0.051774 -0.397505 -0.767869 -0.029420 -0.362672 -0.638149   \n",
      "12 -0.034335  ... -0.035874 -0.409744 -0.583186 -0.003588 -0.368729 -0.509717   \n",
      "13 -0.060219  ... -0.082766 -0.230187 -0.952127 -0.100748 -0.251463 -0.779070   \n",
      "14 -0.060785  ... -0.062156 -0.326105 -0.407744 -0.044001 -0.268598 -0.353827   \n",
      "15  0.093895  ... -0.005978 -0.564901 -0.799120 -0.010991 -0.567881 -0.639503   \n",
      "16 -0.115007  ...  0.044215 -0.194175 -0.488379  0.057241 -0.138051 -0.445781   \n",
      "17  0.016920  ... -0.173064 -0.337297 -0.651959 -0.202164 -0.318872 -0.535075   \n",
      "18 -0.212493  ... -0.079064  0.346972 -0.607817 -0.147219  0.253649 -0.446290   \n",
      "19 -0.195859  ... -0.127764  0.187241 -0.313816 -0.202016  0.129108 -0.216124   \n",
      "20 -0.211251  ... -0.010772 -0.228364 -0.811215 -0.079394 -0.181585 -0.613978   \n",
      "21 -0.292401  ... -0.292588  0.073536  0.189687 -0.422379  0.074837  0.136992   \n",
      "22 -0.144543  ... -0.037782  0.142707 -0.891683 -0.094421  0.107109 -0.683229   \n",
      "23 -0.212836  ... -0.158470  0.178217 -0.435222 -0.245303  0.133854 -0.285933   \n",
      "24 -0.142940  ... -0.027433  0.051616 -0.912103 -0.086352  0.022611 -0.688712   \n",
      "25 -0.043011  ... -0.113661 -0.116671 -0.938735 -0.171306 -0.108271 -0.709889   \n",
      "26 -0.000839  ... -0.104468 -0.234153 -0.973458 -0.166290 -0.210690 -0.725988   \n",
      "\n",
      "       lm_59     lm_60     lm_61     lm_62  \n",
      "0  -0.206388 -0.427605 -0.640160 -0.214185  \n",
      "1  -0.116441  0.019289 -0.415433 -0.064800  \n",
      "2  -0.181825 -0.484332 -0.684320 -0.156854  \n",
      "3  -0.089754 -0.141225 -0.581764 -0.011130  \n",
      "4  -0.064777 -0.129824 -0.879502 -0.057469  \n",
      "5  -0.128748 -0.389604 -0.394978 -0.112971  \n",
      "6  -0.107737 -0.639897 -0.727116 -0.133197  \n",
      "7  -0.113728 -0.183065 -0.546906 -0.056364  \n",
      "8  -0.102222 -0.090922 -0.619946 -0.090985  \n",
      "9   0.025853 -0.054032 -0.430889  0.035792  \n",
      "10 -0.011754 -0.293265 -0.338343  0.011421  \n",
      "11  0.003813 -0.331136 -0.537427  0.030192  \n",
      "12  0.034154 -0.334104 -0.432446  0.061845  \n",
      "13 -0.079354 -0.218915 -0.655982 -0.052629  \n",
      "14 -0.022502 -0.238553 -0.307533 -0.010034  \n",
      "15  0.017505 -0.516339 -0.527670  0.051251  \n",
      "16  0.079440 -0.130780 -0.386826  0.094942  \n",
      "17 -0.181093 -0.248816 -0.476417 -0.162669  \n",
      "18 -0.110567  0.223940 -0.450514 -0.071155  \n",
      "19 -0.191803  0.150389 -0.242315 -0.171670  \n",
      "20 -0.045854 -0.204746 -0.544413 -0.003640  \n",
      "21 -0.420158  0.124147 -0.027634 -0.396578  \n",
      "22 -0.045953  0.084884 -0.625815  0.007370  \n",
      "23 -0.238171  0.142582 -0.340309 -0.221856  \n",
      "24 -0.032527  0.022428 -0.622948  0.026415  \n",
      "25 -0.111857 -0.033657 -0.658056 -0.046820  \n",
      "26 -0.103827 -0.111072 -0.686439 -0.032965  \n",
      "\n",
      "[27 rows x 64 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def load_landmarks_to_dataframe(dataset_dir: str):\n",
    "    dataset_path = Path(dataset_dir)\n",
    "    data = []\n",
    "\n",
    "    # Recorre cada categoría de gesto\n",
    "    for gesture_dir in dataset_path.iterdir():\n",
    "        if not gesture_dir.is_dir():\n",
    "            continue\n",
    "        label = gesture_dir.name\n",
    "\n",
    "        # Recorre todos los archivos npz de landmarks\n",
    "        for npz_file in gesture_dir.glob(\"*_landmarks.npz\"):\n",
    "            try:\n",
    "                npz = np.load(npz_file)\n",
    "                landmarks = npz['landmarks']  # shape: (num_hands, num_landmarks, 3)\n",
    "\n",
    "                # Tomamos solo la primera mano si hay varias\n",
    "                lm = landmarks[0] if landmarks.shape[0] > 0 else np.zeros((21, 3))\n",
    "                \n",
    "                #TODO: Quitar\n",
    "                lm = preprocess_landmarks(lm)\n",
    "\n",
    "                # Aplanar los landmarks a una sola fila\n",
    "                lm_flat = lm.flatten()\n",
    "\n",
    "                # Guardar como diccionario\n",
    "                sample = {\"label\": label}\n",
    "                for i, coord in enumerate(lm_flat):\n",
    "                    sample[f\"lm_{i}\"] = coord\n",
    "\n",
    "                data.append(sample)\n",
    "            except Exception as e:\n",
    "                print(f\"No se pudo cargar {npz_file}: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "df = load_landmarks_to_dataframe(\"gestures_dataset\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ef8266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['left' 'none' 'right']\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     40\u001b[39m model.compile(\n\u001b[32m     41\u001b[39m     optimizer=\u001b[33m'\u001b[39m\u001b[33madam\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     42\u001b[39m     loss=\u001b[33m'\u001b[39m\u001b[33msparse_categorical_crossentropy\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     43\u001b[39m     metrics=[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     44\u001b[39m )\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# 3️⃣ Entrenar\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\n\u001b[32m     52\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# 4️⃣ Evaluar\u001b[39;00m\n\u001b[32m     55\u001b[39m val_loss, val_acc = model.evaluate(X_val, y_val, verbose=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:399\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    398\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:241\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    239\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    240\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    243\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:889\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    886\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    887\u001b[39m   \u001b[38;5;66;03m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[32m    888\u001b[39m   initializers = []\n\u001b[32m--> \u001b[39m\u001b[32m889\u001b[39m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_initializers_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitializers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    891\u001b[39m   \u001b[38;5;66;03m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[32m    892\u001b[39m   \u001b[38;5;66;03m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[32m    893\u001b[39m   \u001b[38;5;28mself\u001b[39m._lock.release()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:696\u001b[39m, in \u001b[36mFunction._initialize\u001b[39m\u001b[34m(self, args, kwds, add_initializers_to)\u001b[39m\n\u001b[32m    691\u001b[39m \u001b[38;5;28mself\u001b[39m._variable_creation_config = \u001b[38;5;28mself\u001b[39m._generate_scoped_tracing_options(\n\u001b[32m    692\u001b[39m     variable_capturing_scope,\n\u001b[32m    693\u001b[39m     tracing_compilation.ScopeType.VARIABLE_CREATION,\n\u001b[32m    694\u001b[39m )\n\u001b[32m    695\u001b[39m \u001b[38;5;66;03m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m696\u001b[39m \u001b[38;5;28mself\u001b[39m._concrete_variable_creation_fn = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrace_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvalid_creator_scope\u001b[39m(*unused_args, **unused_kwds):\n\u001b[32m    701\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:178\u001b[39m, in \u001b[36mtrace_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    175\u001b[39m     args = tracing_options.input_signature\n\u001b[32m    176\u001b[39m     kwargs = {}\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m   concrete_function = \u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options.bind_graph_to_function:\n\u001b[32m    183\u001b[39m   concrete_function._garbage_collector.release()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:283\u001b[39m, in \u001b[36m_maybe_define_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    282\u001b[39m   target_func_type = lookup_func_type\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m concrete_function = \u001b[43m_create_concrete_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_func_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_func_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tracing_options.function_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    288\u001b[39m   tracing_options.function_cache.add(\n\u001b[32m    289\u001b[39m       concrete_function, current_func_context\n\u001b[32m    290\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:310\u001b[39m, in \u001b[36m_create_concrete_function\u001b[39m\u001b[34m(function_type, type_context, func_graph, tracing_options)\u001b[39m\n\u001b[32m    303\u001b[39m   placeholder_bound_args = function_type.placeholder_arguments(\n\u001b[32m    304\u001b[39m       placeholder_context\n\u001b[32m    305\u001b[39m   )\n\u001b[32m    307\u001b[39m disable_acd = tracing_options.attributes \u001b[38;5;129;01mand\u001b[39;00m tracing_options.attributes.get(\n\u001b[32m    308\u001b[39m     attributes_lib.DISABLE_ACD, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    309\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m traced_func_graph = \u001b[43mfunc_graph_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpython_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_control_dependencies\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_acd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43marg_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction_type_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_arg_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_placeholders\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m transform.apply_func_graph_transforms(traced_func_graph)\n\u001b[32m    324\u001b[39m graph_capture_container = traced_func_graph.function_captures\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/framework/func_graph.py:1060\u001b[39m, in \u001b[36mfunc_graph_from_py_func\u001b[39m\u001b[34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[39m\n\u001b[32m   1057\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[32m   1059\u001b[39m _, original_func = tf_decorator.unwrap(python_func)\n\u001b[32m-> \u001b[39m\u001b[32m1060\u001b[39m func_outputs = \u001b[43mpython_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1062\u001b[39m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[32m   1063\u001b[39m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[32m   1064\u001b[39m func_outputs = variable_utils.convert_variables_to_tensors(func_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:599\u001b[39m, in \u001b[36mFunction._generate_scoped_tracing_options.<locals>.wrapped_fn\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m    595\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m default_graph._variable_creator_scope(scope, priority=\u001b[32m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m    596\u001b[39m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[32m    597\u001b[39m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[32m    598\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m     out = \u001b[43mweak_wrapped_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__wrapped__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    600\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py:41\u001b[39m, in \u001b[36mpy_func_from_autograph.<locals>.autograph_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Calls a converted version of original_func.\"\"\"\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m      \u001b[49m\u001b[43moriginal_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mConversionOptions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m          \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m          \u001b[49m\u001b[43moptional_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m          \u001b[49m\u001b[43muser_requested\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[32m     51\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mag_error_metadata\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py:339\u001b[39m, in \u001b[36mconverted_call\u001b[39m\u001b[34m(f, args, kwargs, caller_fn_scope, options)\u001b[39m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_autograph_artifact(f):\n\u001b[32m    338\u001b[39m   logging.log(\u001b[32m2\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mPermanently allowed: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m: AutoGraph artifact\u001b[39m\u001b[33m'\u001b[39m, f)\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_unconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# If this is a partial, unwrap it and redo all the checks.\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, functools.partial):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py:459\u001b[39m, in \u001b[36m_call_unconverted\u001b[39m\u001b[34m(f, args, kwargs, options, update_cache)\u001b[39m\n\u001b[32m    456\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m f.\u001b[34m__self__\u001b[39m.call(args, kwargs)\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m f(*args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py:643\u001b[39m, in \u001b[36mdo_not_convert.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    641\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    642\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:154\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.multi_step_on_iterator\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;129m@tf\u001b[39m.autograph.experimental.do_not_convert\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmulti_step_on_iterator\u001b[39m(iterator):\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.steps_per_execution == \u001b[32m1\u001b[39m:\n\u001b[32m    153\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m tf.experimental.Optional.from_value(\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m             \u001b[43mone_step_on_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m         )\n\u001b[32m    157\u001b[39m     \u001b[38;5;66;03m# the spec is set lazily during the tracing of `tf.while_loop`\u001b[39;00m\n\u001b[32m    158\u001b[39m     empty_outputs = tf.experimental.Optional.empty(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:125\u001b[39m, in \u001b[36mTensorFlowTrainer._autoconvert_optionals.<locals>.wrapper\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(step_func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(data):\n\u001b[32m    119\u001b[39m     converted_data = tree.map_structure(\n\u001b[32m    120\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m i: (\n\u001b[32m    121\u001b[39m             \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(i, tf.experimental.Optional) \u001b[38;5;28;01melse\u001b[39;00m i\n\u001b[32m    122\u001b[39m         ),\n\u001b[32m    123\u001b[39m         data,\n\u001b[32m    124\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     result = \u001b[43mstep_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:906\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[32m    903\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    904\u001b[39m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[32m    905\u001b[39m     \u001b[38;5;66;03m# no_variable_creation function.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m906\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    910\u001b[39m   bound_args = \u001b[38;5;28mself\u001b[39m._concrete_variable_creation_fn.function_type.bind(\n\u001b[32m    911\u001b[39m       *args, **kwds\n\u001b[32m    912\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:132\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    130\u001b[39m args = args \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[32m    131\u001b[39m kwargs = kwargs \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m function = \u001b[43mtrace_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# Bind it ourselves to skip unnecessary canonicalization of default call.\u001b[39;00m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:178\u001b[39m, in \u001b[36mtrace_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    175\u001b[39m     args = tracing_options.input_signature\n\u001b[32m    176\u001b[39m     kwargs = {}\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m   concrete_function = \u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options.bind_graph_to_function:\n\u001b[32m    183\u001b[39m   concrete_function._garbage_collector.release()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:283\u001b[39m, in \u001b[36m_maybe_define_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    282\u001b[39m   target_func_type = lookup_func_type\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m concrete_function = \u001b[43m_create_concrete_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_func_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_func_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tracing_options.function_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    288\u001b[39m   tracing_options.function_cache.add(\n\u001b[32m    289\u001b[39m       concrete_function, current_func_context\n\u001b[32m    290\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:310\u001b[39m, in \u001b[36m_create_concrete_function\u001b[39m\u001b[34m(function_type, type_context, func_graph, tracing_options)\u001b[39m\n\u001b[32m    303\u001b[39m   placeholder_bound_args = function_type.placeholder_arguments(\n\u001b[32m    304\u001b[39m       placeholder_context\n\u001b[32m    305\u001b[39m   )\n\u001b[32m    307\u001b[39m disable_acd = tracing_options.attributes \u001b[38;5;129;01mand\u001b[39;00m tracing_options.attributes.get(\n\u001b[32m    308\u001b[39m     attributes_lib.DISABLE_ACD, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    309\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m traced_func_graph = \u001b[43mfunc_graph_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpython_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_control_dependencies\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_acd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m    \u001b[49m\u001b[43marg_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction_type_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_arg_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_placeholders\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    322\u001b[39m transform.apply_func_graph_transforms(traced_func_graph)\n\u001b[32m    324\u001b[39m graph_capture_container = traced_func_graph.function_captures\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/framework/func_graph.py:1060\u001b[39m, in \u001b[36mfunc_graph_from_py_func\u001b[39m\u001b[34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[39m\n\u001b[32m   1057\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[32m   1059\u001b[39m _, original_func = tf_decorator.unwrap(python_func)\n\u001b[32m-> \u001b[39m\u001b[32m1060\u001b[39m func_outputs = \u001b[43mpython_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1062\u001b[39m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[32m   1063\u001b[39m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[32m   1064\u001b[39m func_outputs = variable_utils.convert_variables_to_tensors(func_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:599\u001b[39m, in \u001b[36mFunction._generate_scoped_tracing_options.<locals>.wrapped_fn\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m    595\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m default_graph._variable_creator_scope(scope, priority=\u001b[32m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m    596\u001b[39m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[32m    597\u001b[39m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[32m    598\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m     out = \u001b[43mweak_wrapped_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__wrapped__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    600\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py:41\u001b[39m, in \u001b[36mpy_func_from_autograph.<locals>.autograph_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Calls a converted version of original_func.\"\"\"\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m      \u001b[49m\u001b[43moriginal_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m      \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconverter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mConversionOptions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m          \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m          \u001b[49m\u001b[43moptional_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m          \u001b[49m\u001b[43muser_requested\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[32m     51\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mag_error_metadata\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py:331\u001b[39m, in \u001b[36mconverted_call\u001b[39m\u001b[34m(f, args, kwargs, caller_fn_scope, options)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conversion.is_in_allowlist_cache(f, options):\n\u001b[32m    330\u001b[39m   logging.log(\u001b[32m2\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mAllowlisted \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m: from cache\u001b[39m\u001b[33m'\u001b[39m, f)\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_unconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ag_ctx.control_status_ctx().status == ag_ctx.Status.DISABLED:\n\u001b[32m    334\u001b[39m   logging.log(\u001b[32m2\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mAllowlisted: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m: AutoGraph is disabled in context\u001b[39m\u001b[33m'\u001b[39m, f)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py:459\u001b[39m, in \u001b[36m_call_unconverted\u001b[39m\u001b[34m(f, args, kwargs, options, update_cache)\u001b[39m\n\u001b[32m    456\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m f.\u001b[34m__self__\u001b[39m.call(args, kwargs)\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m f(*args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py:643\u001b[39m, in \u001b[36mdo_not_convert.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    641\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    642\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:134\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.one_step_on_data\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;129m@tf\u001b[39m.autograph.experimental.do_not_convert\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mone_step_on_data\u001b[39m(data):\n\u001b[32m    133\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    135\u001b[39m     outputs = reduce_per_replica(\n\u001b[32m    136\u001b[39m         outputs,\n\u001b[32m    137\u001b[39m         \u001b[38;5;28mself\u001b[39m.distribute_strategy,\n\u001b[32m    138\u001b[39m         reduction=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    139\u001b[39m     )\n\u001b[32m    140\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py:1673\u001b[39m, in \u001b[36mStrategyBase.run\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   1668\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scope():\n\u001b[32m   1669\u001b[39m   \u001b[38;5;66;03m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[32m   1670\u001b[39m   \u001b[38;5;66;03m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[32m   1671\u001b[39m   fn = autograph.tf_convert(\n\u001b[32m   1672\u001b[39m       fn, autograph_ctx.control_status_ctx(), convert_by_default=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1673\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_extended\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_for_each_replica\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py:3263\u001b[39m, in \u001b[36mStrategyExtendedV1.call_for_each_replica\u001b[39m\u001b[34m(self, fn, args, kwargs)\u001b[39m\n\u001b[32m   3261\u001b[39m   kwargs = {}\n\u001b[32m   3262\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._container_strategy().scope():\n\u001b[32m-> \u001b[39m\u001b[32m3263\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_for_each_replica\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/distribute/distribute_lib.py:4061\u001b[39m, in \u001b[36m_DefaultDistributionExtended._call_for_each_replica\u001b[39m\u001b[34m(self, fn, args, kwargs)\u001b[39m\n\u001b[32m   4059\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call_for_each_replica\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[32m   4060\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ReplicaContext(\u001b[38;5;28mself\u001b[39m._container_strategy(), replica_id_in_sync_group=\u001b[32m0\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m4061\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py:643\u001b[39m, in \u001b[36mdo_not_convert.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    641\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    642\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx.ControlStatusCtx(status=ag_ctx.Status.DISABLED):\n\u001b[32m--> \u001b[39m\u001b[32m643\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:81\u001b[39m, in \u001b[36mTensorFlowTrainer.train_step\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trainable_weights:\n\u001b[32m     80\u001b[39m     trainable_weights = \u001b[38;5;28mself\u001b[39m.trainable_weights\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     gradients = \u001b[43mtape\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m     \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28mself\u001b[39m.optimizer.apply_gradients(\u001b[38;5;28mzip\u001b[39m(gradients, trainable_weights))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/eager/backprop.py:1066\u001b[39m, in \u001b[36mGradientTape.gradient\u001b[39m\u001b[34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[39m\n\u001b[32m   1060\u001b[39m   output_gradients = (\n\u001b[32m   1061\u001b[39m       composite_tensor_gradient.get_flat_tensors_for_gradients(\n\u001b[32m   1062\u001b[39m           output_gradients))\n\u001b[32m   1063\u001b[39m   output_gradients = [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops.convert_to_tensor(x)\n\u001b[32m   1064\u001b[39m                       \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m output_gradients]\n\u001b[32m-> \u001b[39m\u001b[32m1066\u001b[39m flat_grad = \u001b[43mimperative_grad\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimperative_grad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1068\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_targets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1069\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_sources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1070\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1071\u001b[39m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[43m=\u001b[49m\u001b[43mflat_sources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1072\u001b[39m \u001b[43m    \u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m=\u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1074\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._persistent:\n\u001b[32m   1075\u001b[39m   \u001b[38;5;66;03m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[32m   1076\u001b[39m   \u001b[38;5;28mself\u001b[39m._watched_variables = \u001b[38;5;28mself\u001b[39m._tape.watched_variables()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[39m, in \u001b[36mimperative_grad\u001b[39m\u001b[34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m     64\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     65\u001b[39m       \u001b[33m\"\u001b[39m\u001b[33mUnknown value for unconnected_gradients: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m % unconnected_gradients)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_TapeGradient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtape\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_tape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_gradients\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43msources_raw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43munconnected_gradients\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/eager/backprop.py:148\u001b[39m, in \u001b[36m_gradient_function\u001b[39m\u001b[34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[39m\n\u001b[32m    146\u001b[39m     gradient_name_scope += forward_pass_name_scope + \u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    147\u001b[39m   \u001b[38;5;28;01mwith\u001b[39;00m ops.name_scope(gradient_name_scope):\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgrad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmock_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mout_grads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m grad_fn(mock_op, *out_grads)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/ops/nn_grad.py:414\u001b[39m, in \u001b[36m_ReluGrad\u001b[39m\u001b[34m(op, grad)\u001b[39m\n\u001b[32m    412\u001b[39m \u001b[38;5;129m@ops\u001b[39m.RegisterGradient(\u001b[33m\"\u001b[39m\u001b[33mRelu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    413\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_ReluGrad\u001b[39m(op: ops.Operation, grad):\n\u001b[32m--> \u001b[39m\u001b[32m414\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_nn_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrelu_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/ops/gen_nn_ops.py:11797\u001b[39m, in \u001b[36mrelu_grad\u001b[39m\u001b[34m(gradients, features, name)\u001b[39m\n\u001b[32m  11795\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[32m  11796\u001b[39m \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[32m> \u001b[39m\u001b[32m11797\u001b[39m _, _, _op, _outputs = \u001b[43m_op_def_library\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply_op_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m  11798\u001b[39m \u001b[43m      \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mReluGrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m  11799\u001b[39m _result = _outputs[:]\n\u001b[32m  11800\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _execute.must_record_gradient():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/framework/op_def_library.py:796\u001b[39m, in \u001b[36m_apply_op_helper\u001b[39m\u001b[34m(op_type_name, name, **keywords)\u001b[39m\n\u001b[32m    791\u001b[39m must_colocate_inputs = [val \u001b[38;5;28;01mfor\u001b[39;00m arg, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(op_def.input_arg, inputs)\n\u001b[32m    792\u001b[39m                         \u001b[38;5;28;01mif\u001b[39;00m arg.is_ref]\n\u001b[32m    793\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _MaybeColocateWith(must_colocate_inputs):\n\u001b[32m    794\u001b[39m   \u001b[38;5;66;03m# Add Op to graph\u001b[39;00m\n\u001b[32m    795\u001b[39m   \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m796\u001b[39m   op = \u001b[43mg\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_create_op_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_type_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscope\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattr_protos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m=\u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[38;5;66;03m# `outputs` is returned as a separate return value so that the output\u001b[39;00m\n\u001b[32m    801\u001b[39m \u001b[38;5;66;03m# tensors can the `op` per se can be decoupled so that the\u001b[39;00m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# `op_callbacks` can function properly. See framework/op_callbacks.py\u001b[39;00m\n\u001b[32m    803\u001b[39m \u001b[38;5;66;03m# for more details.\u001b[39;00m\n\u001b[32m    804\u001b[39m outputs = op.outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/projects/python/DeepGestureGame/venv/lib/python3.12/site-packages/tensorflow/python/framework/func_graph.py:613\u001b[39m, in \u001b[36mFuncGraph._create_op_internal\u001b[39m\u001b[34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[39m\n\u001b[32m    611\u001b[39m     inp = ctxt.AddValue(inp)\n\u001b[32m    612\u001b[39m   inp = \u001b[38;5;28mself\u001b[39m.capture(inp)\n\u001b[32m--> \u001b[39m\u001b[32m613\u001b[39m   captured_inputs.append(inp)\n\u001b[32m    614\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()._create_op_internal(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[32m    615\u001b[39m     op_type, captured_inputs, dtypes, input_types, name, attrs, op_def,\n\u001b[32m    616\u001b[39m     compute_device)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# 1️⃣ Cargar DataFrame de landmarks\n",
    "df = load_landmarks_to_dataframe(\"gestures_dataset\")\n",
    "\n",
    "# Todo lo que no sea right o left -> none\n",
    "df['label'] = df['label'].apply(lambda x: x if x in ['right', 'left'] else 'none')\n",
    "\n",
    "# Separar características y etiquetas\n",
    "X = df.drop(columns=['label']).values.astype('float32')\n",
    "y = df['label'].values\n",
    "\n",
    "# Escalar características\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Codificar etiquetas a números\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)  # right=2, left=1, none=0, por ejemplo\n",
    "\n",
    "# Dividir en train/validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "\n",
    "# 2️⃣ Construir modelo MLP\n",
    "input_size = X.shape[1]  # 63 landmarks\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(input_size,)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(3, activation='softmax')  # 3 clases: right, left, none\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# 3️⃣ Entrenar\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# 4️⃣ Evaluar\n",
    "val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# 5️⃣ Predecir\n",
    "def predict_gesture(landmarks_vector):\n",
    "    lm_vector = np.array(landmarks_vector).reshape(1, -1).astype('float32')\n",
    "    lm_vector = scaler.transform(lm_vector)\n",
    "    pred = model.predict(lm_vector)\n",
    "    class_idx = np.argmax(pred)\n",
    "    return le.inverse_transform([class_idx])[0]\n",
    "\n",
    "# Ejemplo:\n",
    "# gesture = predict_gesture(new_landmarks_vector)\n",
    "# print(\"Predicción:\", gesture)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "30f7a23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1765038641.671502  119354 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1765038641.673975  133766 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.0.7-0ubuntu0.24.04.2), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1765038641.704448  133758 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1765038641.718373  133757 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "1\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "1\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "1\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "1\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "1\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "1\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "1\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "1\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "1\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "1\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "2\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "1\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "1\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "hands_model = SafeHands(min_detection_confidence=0.8, min_tracking_confidence=0.5, max_num_hands=1)\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "def loop(hands: SafeHands, frame: np.ndarray) -> bool:\n",
    "\tframe_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\tresult = hands.process(frame_rgb)\n",
    "\tif result:\n",
    "\t\thands_landmarks = result[0]\n",
    "\t\tfor hand_landmarks in hands_landmarks:\n",
    "\t\t\tprint(predict_gesture(preprocess_landmarks(hand_landmarks)))\n",
    "\treturn False\n",
    "\n",
    "video_capture_loop(\n",
    "    cap=video_capture,\n",
    "    context=hands_model,\n",
    "    loop=loop,\n",
    "    flipH=True,\n",
    "    show_fps=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fe044cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1765036005.987334  119354 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1765036005.990308  119872 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.0.7-0ubuntu0.24.04.2), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1765036006.023694  119862 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1765036006.043856  119869 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "hands_model = SafeHands(min_detection_confidence=0.8, min_tracking_confidence=0.5, max_num_hands=1)\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "def loop(hands: SafeHands, frame: np.ndarray) -> bool:\n",
    "\tframe_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\tresult = hands.process(frame_rgb)\n",
    "\tif result:\n",
    "\t\tlandmarks = result[0]\n",
    "\t\tfor hand_landmarks in landmarks:\n",
    "\t\t\tdraw_overlays(frame, hand_landmarks, preprocess_landmarks(hand_landmarks))\n",
    "\treturn False\n",
    "\n",
    "video_capture_loop(\n",
    "    cap=video_capture,\n",
    "    context=hands_model,\n",
    "    loop=loop,\n",
    "    flipH=True,\n",
    "    show_fps=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2864fa6",
   "metadata": {},
   "source": [
    "# Real-Time Hand Landmark Preprocessing Visualization\n",
    "\n",
    "This cell captures video from the webcam, processes hand landmarks in real-time, and visualizes both the original landmarks and the preprocessed coordinates.\n",
    "\n",
    "## Main Components\n",
    "\n",
    "1. **Video Capture**\n",
    "    - `cv2.VideoCapture(0)` initializes the webcam.  \n",
    "    - Frames are continuously captured in a loop until the user quits.\n",
    "\n",
    "2. **Hand Detection**\n",
    "    - MediaPipe's `Hands` module is used with:\n",
    "        - `min_detection_confidence=0.8`\n",
    "        - `min_tracking_confidence=0.5`  \n",
    "    - The `hands.process(frame_rgb)` method detects hand landmarks in each frame.\n",
    "\n",
    "3. **Preprocessing Pipeline**\n",
    "    - For each detected hand:\n",
    "        1. Extract landmarks.\n",
    "        2. Normalize wrist position.\n",
    "        3. Scale landmarks relative to hand size.\n",
    "        4. Normalize values to the range [-1, 1].\n",
    "    - This is done using the `preprocess_landmarks` function.\n",
    "\n",
    "4. **Overlay Visualization**\n",
    "    - `draw_overlays` draws both:\n",
    "        - MediaPipe hand landmarks.\n",
    "        - Preprocessed coordinates as text annotations.\n",
    "\n",
    "5. **Display**\n",
    "    - The annotated frame is shown in a window using `cv2.imshow`.\n",
    "    - Press `q` to quit the visualization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
