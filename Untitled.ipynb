{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ddc54549-8c2c-4219-9330-03c7d5d14c4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mediapipe in ./venv/lib/python3.12/site-packages (0.10.21)\n",
      "Requirement already satisfied: opencv-python in ./venv/lib/python3.12/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: absl-py in ./venv/lib/python3.12/site-packages (from mediapipe) (2.3.1)\n",
      "Requirement already satisfied: attrs>=19.1.0 in ./venv/lib/python3.12/site-packages (from mediapipe) (25.4.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in ./venv/lib/python3.12/site-packages (from mediapipe) (25.9.23)\n",
      "Requirement already satisfied: jax in ./venv/lib/python3.12/site-packages (from mediapipe) (0.7.1)\n",
      "Requirement already satisfied: jaxlib in ./venv/lib/python3.12/site-packages (from mediapipe) (0.7.1)\n",
      "Requirement already satisfied: matplotlib in ./venv/lib/python3.12/site-packages (from mediapipe) (3.10.7)\n",
      "Requirement already satisfied: numpy<2 in ./venv/lib/python3.12/site-packages (from mediapipe) (1.26.4)\n",
      "Requirement already satisfied: opencv-contrib-python in ./venv/lib/python3.12/site-packages (from mediapipe) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in ./venv/lib/python3.12/site-packages (from mediapipe) (4.25.8)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in ./venv/lib/python3.12/site-packages (from mediapipe) (0.5.3)\n",
      "Requirement already satisfied: sentencepiece in ./venv/lib/python3.12/site-packages (from mediapipe) (0.2.1)\n",
      "Requirement already satisfied: CFFI>=1.0 in ./venv/lib/python3.12/site-packages (from sounddevice>=0.4.4->mediapipe) (2.0.0)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in ./venv/lib/python3.12/site-packages (from jax->mediapipe) (0.5.4)\n",
      "Requirement already satisfied: opt_einsum in ./venv/lib/python3.12/site-packages (from jax->mediapipe) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.12 in ./venv/lib/python3.12/site-packages (from jax->mediapipe) (1.16.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.12/site-packages (from matplotlib->mediapipe) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.12/site-packages (from matplotlib->mediapipe) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.12/site-packages (from matplotlib->mediapipe) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./venv/lib/python3.12/site-packages (from matplotlib->mediapipe) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.12/site-packages (from matplotlib->mediapipe) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./venv/lib/python3.12/site-packages (from matplotlib->mediapipe) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in ./venv/lib/python3.12/site-packages (from matplotlib->mediapipe) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.12/site-packages (from matplotlib->mediapipe) (2.9.0.post0)\n",
      "Requirement already satisfied: pycparser in ./venv/lib/python3.12/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.23)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "834c6e73-a4f7-4059-9b4e-ccde91a5b69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import uuid\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880c9683",
   "metadata": {},
   "source": [
    "# Hand Landmark Preprocessing Pipeline\n",
    "\n",
    "This notebook cell contains a set of functions to **preprocess hand landmarks**.  \n",
    "The preprocessing pipeline includes the following steps:\n",
    "\n",
    "1. **Extract coordinates** – Convert each hand landmark into a flat array of `(x, y, z)` values.  \n",
    "2. **Normalize wrist position** – Shift all landmarks so that the wrist (the first landmark) is at the origin `(0,0,0)`.  \n",
    "3. **Scale according to hand size** – Adjust the landmarks so that hand size differences do not affect the analysis.  \n",
    "4. **Normalize values** – Scale all coordinates to a fixed range (e.g., `[-1, 1]`) for consistent model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c253e9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_landmarks(landmarks: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Shifts all landmarks so that the wrist (first landmark) is at the origin (0,0,0).\n",
    "\n",
    "    Parameters:\n",
    "        landmarks: np.ndarray of shape [NUM_LANDMARKS, 3]\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Landmarks translated relative to the wrist.\n",
    "    \"\"\"\n",
    "    landmarks = landmarks.copy()\n",
    "    base = landmarks[0]  # [x, y, z] del primer landmark (muñeca)\n",
    "    return landmarks - base\n",
    "\n",
    "\n",
    "def scale_landmarks(landmarks: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalizes the scale of the hand landmarks based on the distance from wrist to middle finger MCP joint.\n",
    "\n",
    "    Parameters:\n",
    "        landmarks: np.ndarray de landmarks normalizados por la muñeca [NUM_LANDMARKS, 3]\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Landmarks escalados a tamaño de mano consistente.\n",
    "    \"\"\"\n",
    "    wrist = landmarks[0]\n",
    "    mcp = landmarks[9]  # 9 es el MCP del dedo medio\n",
    "    scale = np.linalg.norm(mcp - wrist)\n",
    "    return landmarks / scale\n",
    "\n",
    "\n",
    "def minmax_landmarks(landmarks: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalizes all landmark values to the range [-1, 1] based on the maximum absolute value.\n",
    "\n",
    "    Parameters:\n",
    "        landmarks: np.ndarray de landmarks escalados\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Landmarks normalizados a [-1, 1]\n",
    "    \"\"\"\n",
    "    max_val = np.max(np.abs(landmarks))\n",
    "    return landmarks / max_val\n",
    "\n",
    "\n",
    "def preprocess_landmarks(hand_landmarks: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applies the full preprocessing pipeline, preserving [NUM_LANDMARKS, 3] shape.\n",
    "\n",
    "    Parameters:\n",
    "        hand_landmarks: Array de forma [21, 3]\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Fully preprocessed landmark array.\n",
    "    \"\"\"\n",
    "    landmarks = normalize_landmarks(hand_landmarks)\n",
    "    landmarks = scale_landmarks(landmarks)\n",
    "    landmarks = minmax_landmarks(landmarks)\n",
    "    return landmarks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0203ed2b",
   "metadata": {},
   "source": [
    "# Hand Landmark Drawing and Overlay Function\n",
    "\n",
    "This function draws hand landmarks on a video frame and overlays the processed landmark coordinates as text annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc05b654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands_connections = mp.solutions.hands.HAND_CONNECTIONS\n",
    "\n",
    "def draw_overlays(frame, hand_landmarks_np, processed_landmarks=None):\n",
    "    \"\"\"\n",
    "    Draws hand landmarks and connections on a given frame.\n",
    "    Parameters:\n",
    "        frame (np.array): The image frame.\n",
    "        hand_landmarks_np (np.array): Numpy array of shape (21, 3) with hand landmarks.\n",
    "        processed_landmarks (np.array, optional): Preprocessed landmark coordinates [21,3].\n",
    "    \"\"\"\n",
    "    h, w, _ = frame.shape\n",
    "\n",
    "    for connection in mp_hands_connections:\n",
    "        start_idx, end_idx = connection\n",
    "        x0, y0 = int(hand_landmarks_np[start_idx][0] * w), int(hand_landmarks_np[start_idx][1] * h)\n",
    "        x1, y1 = int(hand_landmarks_np[end_idx][0] * w), int(hand_landmarks_np[end_idx][1] * h)\n",
    "        cv2.line(frame, (x0, y0), (x1, y1), (0, 255, 0), 2)\n",
    "\n",
    "    for idx, lm in enumerate(hand_landmarks_np):\n",
    "        cx, cy = int(lm[0] * w), int(lm[1] * h)\n",
    "        cv2.circle(frame, (cx, cy), 6, (0, 0, 255), -1)\n",
    "\n",
    "        if processed_landmarks is not None:\n",
    "            px, py = processed_landmarks[idx][0], processed_landmarks[idx][1]\n",
    "            text = f\"{idx}:({px:.2f},{py:.2f})\"\n",
    "            cv2.putText(frame, text, (cx + 5, cy - 5),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.35, (255, 255, 0), 1, cv2.LINE_AA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dfc8a0",
   "metadata": {},
   "source": [
    "# Flexible Video Capture Loop\n",
    "\n",
    "The `video_capture_loop` function:\n",
    "\n",
    "- Reads frames from a video source (`cv2.VideoCapture`)\n",
    "- Optionally flips frames horizontally or vertically\n",
    "- Processes frames using a user-defined function\n",
    "- Can display frames in a window or skip GUI for automated processing\n",
    "- Ensures proper resource cleanup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6c98a308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import AbstractContextManager\n",
    "from typing import Callable, TypeVar\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "ContextType = TypeVar(\"ContextType\", bound=AbstractContextManager)\n",
    "\n",
    "class ContextError(Exception):\n",
    "    pass\n",
    "\n",
    "class VideoCaptureError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def flip_frame(frame: np.ndarray, flipH: bool, flipV: bool) -> np.ndarray:\n",
    "    if flipH and flipV:\n",
    "        return cv2.flip(frame, -1)\n",
    "    elif flipH:\n",
    "        return cv2.flip(frame, 1) \n",
    "    elif flipV:\n",
    "        return cv2.flip(frame, 0)\n",
    "    return frame\n",
    "\n",
    "\n",
    "def video_capture_loop(\n",
    "    cap: cv2.VideoCapture,\n",
    "    context: ContextType,\n",
    "    loop: Callable[[ContextType, np.ndarray], bool],\n",
    "    flipH: bool = False,\n",
    "    flipV: bool = False,\n",
    "    show_window: bool = True,\n",
    "    show_fps: bool = False\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Continuously captures frames from a video source, processes them using a user-defined \n",
    "    function, optionally flips frames, optionally displays them in a window, and optionally\n",
    "    shows the current FPS on the video.\n",
    "    \"\"\"\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    delay = int(1000 / fps) if fps > 0 else 1\n",
    "    prev_time = time.time()\n",
    "\n",
    "    try:\n",
    "        with context as ctx:\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret or frame is None:\n",
    "                    total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "                    current_frame = cap.get(cv2.CAP_PROP_POS_FRAMES)\n",
    "                    if total_frames > 0 and current_frame >= total_frames:\n",
    "                        break\n",
    "                    else:\n",
    "                        raise VideoCaptureError(\"Unable to read video capture.\")\n",
    "                \n",
    "                frame = flip_frame(frame, flipH, flipV)\n",
    "                \n",
    "                try:\n",
    "                    if loop(ctx, frame):\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    raise VideoCaptureError(\"Error while processing frame.\") from e\n",
    "                \n",
    "                if show_window:\n",
    "                    if show_fps:\n",
    "                        current_time = time.time()\n",
    "                        current_fps = 1.0 / (current_time - prev_time) if (current_time - prev_time) > 0 else 0\n",
    "                        prev_time = current_time\n",
    "                        cv2.putText(\n",
    "                            frame,\n",
    "                            f\"FPS: {current_fps:.2f}\",\n",
    "                            (10, 30),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            1,\n",
    "                            (0, 255, 0),\n",
    "                            2,\n",
    "                            cv2.LINE_AA\n",
    "                        )\n",
    "                    cv2.imshow(\"VideoInput\", frame)\n",
    "                    if cv2.waitKey(delay) & 0xFF == ord(\"q\"):\n",
    "                        break\n",
    "\n",
    "    except Exception as e:\n",
    "        if not isinstance(e, VideoCaptureError):\n",
    "            raise ContextError(\"Error while using context.\") from e\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    finally:\n",
    "        if cap.isOpened():\n",
    "            cap.release()\n",
    "        if show_window:\n",
    "            cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7d0ed19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from asyncio import Protocol\n",
    "import numpy as np\n",
    "from contextlib import AbstractContextManager\n",
    "from typing import Optional, Sequence, Tuple, cast\n",
    "from mediapipe.python.solutions.hands import Hands\n",
    "\n",
    "\n",
    "class SafeHandsResult(Protocol):\n",
    "    multi_hand_landmarks: Optional[Sequence]\n",
    "    multi_hand_world_landmarks: Optional[Sequence]\n",
    "    multi_handedness: Optional[Sequence]\n",
    "\n",
    "class SafeHands(AbstractContextManager):\n",
    "\tdef __init__(self, \n",
    "              \tstatic_image_mode: bool = False,\n",
    "\t\t\t    max_num_hands: int = 2,\n",
    "\t\t\t    model_complexity: int = 1,\n",
    "\t\t\t    min_detection_confidence: float = 0.5,\n",
    "\t\t\t    min_tracking_confidence: float = 0.5):\n",
    "\t\tself.max_num_hands = max_num_hands\n",
    "\t\tself.hands = Hands(static_image_mode = static_image_mode,\n",
    "\t\t\t    max_num_hands = max_num_hands,\n",
    "\t\t\t    model_complexity = model_complexity,\n",
    "\t\t\t    min_detection_confidence = min_detection_confidence,\n",
    "\t\t\t    min_tracking_confidence = min_tracking_confidence)\n",
    "    \n",
    "\tdef __enter__(self) -> \"SafeHands\":\n",
    "\t\treturn self\n",
    "    \n",
    "\tdef __exit__(self, exc_type, exc_value, traceback) -> None:\n",
    "\t\tself.hands.close()\n",
    "\n",
    "\tdef process(self, frame_rgb: np.ndarray) -> Optional[Tuple[np.ndarray, np.ndarray, np.ndarray]]:\n",
    "\t\tmp_result = cast(SafeHandsResult, self.hands.process(frame_rgb))\n",
    "\t\tif (not mp_result.multi_hand_landmarks and not mp_result.multi_hand_world_landmarks and not mp_result.multi_handedness):\n",
    "\t\t\treturn None\n",
    "\t\tlandmarks = np.zeros((self.max_num_hands, 21, 3), dtype=np.float32)\n",
    "\t\tworld_landmarks = np.zeros((self.max_num_hands, 21, 3), dtype=np.float32)\n",
    "\t\thandedness = np.zeros((self.max_num_hands, 2), dtype=np.float32)\n",
    "\t\tif mp_result.multi_hand_landmarks:\n",
    "\t\t\tfor i, hand in enumerate(mp_result.multi_hand_landmarks[:self.max_num_hands]):\n",
    "\t\t\t\tfor j, lm in enumerate(hand.landmark):\n",
    "\t\t\t\t\tlandmarks[i, j, 0] = lm.x\n",
    "\t\t\t\t\tlandmarks[i, j, 1] = lm.y\n",
    "\t\t\t\t\tlandmarks[i, j, 2] = lm.z\n",
    "\n",
    "\t\tif mp_result.multi_hand_world_landmarks:\n",
    "\t\t\tfor i, hand in enumerate(mp_result.multi_hand_world_landmarks[:self.max_num_hands]):\n",
    "\t\t\t\tfor j, lm in enumerate(hand.landmark):\n",
    "\t\t\t\t\tworld_landmarks[i, j, 2] = lm.z\n",
    "\t\t\t\t\tworld_landmarks[i, j, 0] = lm.x\n",
    "\t\t\t\t\tworld_landmarks[i, j, 1] = lm.y\n",
    "\n",
    "\t\tif mp_result.multi_handedness:\n",
    "\t\t\tfor i, h in enumerate(mp_result.multi_handedness[:self.max_num_hands]):\n",
    "\t\t\t\thandedness[i, 0] = 0 if h.classification[0].label == \"Left\" else 1\n",
    "\t\t\t\thandedness[i, 1] = h.classification[0].score\n",
    "\n",
    "\t\treturn landmarks, world_landmarks, handedness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6fe044cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1765030502.409102   77667 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1765030502.411976  109646 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 25.0.7-0ubuntu0.24.04.2), renderer: Mesa Intel(R) UHD Graphics (CML GT2)\n",
      "W0000 00:00:1765030502.442793  109636 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1765030502.453916  109639 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "/tmp/ipykernel_77667/1478771852.py:29: RuntimeWarning: invalid value encountered in divide\n",
      "  return landmarks / scale\n"
     ]
    }
   ],
   "source": [
    "hands_model = SafeHands(min_detection_confidence=0.8, min_tracking_confidence=0.5)\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "def loop(hands: SafeHands, frame: np.ndarray) -> bool:\n",
    "\tframe_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\tresult = hands.process(frame_rgb)\n",
    "\tif result:\n",
    "\t\tlandmarks = result[0]\n",
    "\t\tfor hand_landmarks in landmarks:\n",
    "\t\t\tdraw_overlays(frame, hand_landmarks, preprocess_landmarks(hand_landmarks))\n",
    "\treturn False\n",
    "\n",
    "video_capture_loop(\n",
    "    cap=video_capture,\n",
    "    context=hands_model,\n",
    "    loop=loop,\n",
    "    flipH=True,\n",
    "    show_fps=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2864fa6",
   "metadata": {},
   "source": [
    "# Real-Time Hand Landmark Preprocessing Visualization\n",
    "\n",
    "This cell captures video from the webcam, processes hand landmarks in real-time, and visualizes both the original landmarks and the preprocessed coordinates.\n",
    "\n",
    "## Main Components\n",
    "\n",
    "1. **Video Capture**\n",
    "    - `cv2.VideoCapture(0)` initializes the webcam.  \n",
    "    - Frames are continuously captured in a loop until the user quits.\n",
    "\n",
    "2. **Hand Detection**\n",
    "    - MediaPipe's `Hands` module is used with:\n",
    "        - `min_detection_confidence=0.8`\n",
    "        - `min_tracking_confidence=0.5`  \n",
    "    - The `hands.process(frame_rgb)` method detects hand landmarks in each frame.\n",
    "\n",
    "3. **Preprocessing Pipeline**\n",
    "    - For each detected hand:\n",
    "        1. Extract landmarks.\n",
    "        2. Normalize wrist position.\n",
    "        3. Scale landmarks relative to hand size.\n",
    "        4. Normalize values to the range [-1, 1].\n",
    "    - This is done using the `preprocess_landmarks` function.\n",
    "\n",
    "4. **Overlay Visualization**\n",
    "    - `draw_overlays` draws both:\n",
    "        - MediaPipe hand landmarks.\n",
    "        - Preprocessed coordinates as text annotations.\n",
    "\n",
    "5. **Display**\n",
    "    - The annotated frame is shown in a window using `cv2.imshow`.\n",
    "    - Press `q` to quit the visualization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
